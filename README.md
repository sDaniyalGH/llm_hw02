# llm_hw02
Parameter-Efficient Fine-Tuning (PEFT) with LoRA/QLoRA


· Implemented LoRA (Low-Rank Adaptation) and QLoRA from scratch to fine-tune decoder-only LLMs for Persian
poet classification.

· Analyzed the trade-offs between rank configuration, model complexity, and memory consumption.

· Evaluated model robustness and confusion matrices to identify misclassification patterns.
